\section{Optimal Control of Pitch/Travel with Feedback (LQ)}\label{sec:prob3}
This problem involves implementing an LQ controller for optimal control with feedback. We will calculate a gain matrix K using the LQ controller, implement feedback on the helicopter, and look into if MPC is a good alternative to LQ.

\subsection{Calculating the gain matrix K}
For calculating the gain matrix K we needed to use the matlab function dlqr, which is a linear-quadratic regulator design that minimiizes the cost function: 
\begin{equation}  
J = Sum {x'Qx + u'Ru + 2*x'Nu} 
/end{equation}. 
This function depend on the system matrices A and B aswell as Q and R, which we will need to choose an appropriate weight on. At first we choose Q and R as:

\begin{equation}
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 }
\end{bmatrix}}_{Q}\qquad\boldR= 1
\end{equation}.

 By trial and error we got the best result by penalizing the state for travel a lot more than the pitch. This gave us a new Q matrix on the form:

\begin{equation}
\begin{bmatrix}
50 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 }
\end{bmatrix}}_{Q}
\end{equation}.

\subsection{Implementing feedback on the helicopter}
An implementation of feedback can be seen on the simulink diagram.

%Figure of simulink model with feedback 
By using the gain matrix K calculated in last task, we see that by penalizing travel hard and pitch soft, the helicopter follows the travel trajectory closer than when weighting all states the same.
%Figure of helicopter response for Q_LQR = diag[50 1 1 1]

We also tried penalizing the pitch more than travel, which gave us a bad result. This because the optimal pitch reference trajectory is based on a bad model. 
%Figure of helicopter response for Q_LQR = diag[1 1 10 1]

\subsection{Comparison between LQR and MPC}
The way to implement an MPC controller would be to use the same procedure as for the LQR, but for every time step. Then using the first time step as the input u.

The advantages of using MPC instead of LQR; is that it gives us the possibility to have constraints in the regulator, can potentially produce a trajectory which performs its' task more cheaper and we get a implicit feedback with the use of MPC.
The main disadvantage of using MPC is that the calculations are a lot heavier to perform.
%Figure of figure 8, but with MPC implemented instead (if we have a figure of this)   

% MPC. Fordeler og ulemper.

% + Kan innføre harde begrensninger, til enhver tid. LQR støtter ikke det.
% + Kan potensielt gi en trajektor/pådragssekvens som utfører oppgaven billigere (lavere objektivfunksjonsverdi)
% - Tungt å beregne.


% \section{Notater under dagen}
% Vi satt Q = diag[1 0 0 0], R = 2, og justerte Q_LQR.

% * Q_LQR justerer hvor mye vi vil straffe avvik mellom målt tilstands- og pådrags-trajektor og den beregnede optimale trajektoren.

% * R_LQR justerer hvor hardt vi vil straffe avvik mellom pådrag brukt og optimalt
% pådrag???
% TODO: Sjekk dette!

% Hvordan lage plots
% ------------------
% For hver kjøring med parametere beskrevet nedenfor, så ligger en
% tilsvarende measurements_q_xxxx.mat i measurements mappa. load
% denne i Matlab, plot optimal trajectory og plot målingene.

% Følg prosedyre i niceFigure.m for å plotte figurene i god kvalitet.

% Q_LQR = diag[1 1 1 1], R = diag[1]
% ----------------------------------
% Sammenligningsreferanse.

% Q_LQR = diag[20 1 1 1], R = diag[1]
% -----------------------------------
% Følger travel-trajektoren tettere, gav bra respons.

% Q_LQR = diag[50 1 1 1], R = diag[1]
% -----------------------------------
% Følger travel-trajektoren enda tettere, bedre respons.

% Q_LQR = diag[50 5 0 0], R = diag[1]
% -----------------------------------
% Ble ustabilt når vi tulla med pitch manuelt på slutten, siden
% vi ikke straffer bruk av pitch rate.

% Q_LQR = diag[50 1 20 1], R = diag[1]
% -----------------------------------
% Kommentar?

% Q_LQR = diag[1 1 10 1], R = diag[1]
% ----------------------------------
% Den optimale pitch-referanse trajektoren er basert på en dårlig modell.
% Så vi bør egentlig ikke følge den altfor tett. Følging av travel-trajektoren
% gir bedre resultat og kompenserer for modellfeil.

% Q_LQR = diag[1 1 40 1], R = diag[1]
% ----------------------------------
% Verre respons, siden vi følger en dårlig pådrags-trajektor.
